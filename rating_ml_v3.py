#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë”¥ëŸ¬ë‹ ê¸°ë°˜ ML Rating ì‹œìŠ¤í…œ v3
- LSTM ì‹œê³„ì—´ ì˜ˆì¸¡ (TensorFlow/Keras with GPU acceleration)
- Zone + ê°€ê²© ë™ì‹œ ì˜ˆì¸¡
- N/B Wave ì‹œí€€ìŠ¤ í•™ìŠµ
- GPU ìë™ ê°ì§€ ë° ìµœì í™”
"""

import os
import json
import pickle
import numpy as np
from typing import Dict, List, Optional, Tuple
from pathlib import Path
from datetime import datetime
from utils.logger import setup_logger

# Logger ì„¤ì •
logger = setup_logger('ml_v3', log_dir='logs')
logger.info("=" * 60)
logger.info("[rating_ml_v3] ë”¥ëŸ¬ë‹ ëª¨ë“ˆ ë¡œë“œë¨")
logger.info("=" * 60)

# Optional TensorFlow/Keras imports with GPU support
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Concatenate
    from tensorflow.keras.optimizers import Adam
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import train_test_split
    
    TF_AVAILABLE = True
    
    # GPU ì„¤ì •
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logger.info(f"[ml_v3] âœ… GPU í™œì„±í™”: {len(gpus)}ê°œ GPU ê°ì§€")
            for i, gpu in enumerate(gpus):
                logger.info(f"  GPU {i}: {gpu.name}")
            USE_GPU = True
        except RuntimeError as e:
            logger.warning(f"[ml_v3] GPU ì„¤ì • ì˜¤ë¥˜: {e}")
            USE_GPU = False
    else:
        logger.warning("[ml_v3] âš ï¸ GPU ì—†ìŒ - CPUë¡œ ì‹¤í–‰")
        USE_GPU = False
        
except ImportError:
    TF_AVAILABLE = False
    USE_GPU = False
    logger.warning("[ml_v3] âš ï¸ TensorFlow ì—†ìŒ - ë”¥ëŸ¬ë‹ ê¸°ëŠ¥ ë¹„í™œì„±í™”")


class LSTMPredictionModel:
    """LSTM ê¸°ë°˜ Zone + ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.model: Optional[Model] = None
        self.scaler_x: Optional[MinMaxScaler] = None
        self.scaler_y: Optional[MinMaxScaler] = None
        
        # GPU ìŠ¤ì¼€ì¼ëŸ¬ (TensorFlow ops ì‚¬ìš©)
        self.scaler_x_min = None
        self.scaler_x_range = None
        self.scaler_y_min = None
        self.scaler_y_range = None
        
        self.meta: Dict = {}
        
        self.sequence_length = 30  # 30ê°œ ì‹œì  ì‹œí€€ìŠ¤
        self.prediction_horizon = 10  # 10ê°œ ë¯¸ë˜ ì˜ˆì¸¡
        
        self.model_path = self.model_dir / "lstm_model.h5"
        self.scaler_x_path = self.model_dir / "lstm_scaler_x.pkl"
        self.scaler_y_path = self.model_dir / "lstm_scaler_y.pkl"
        self.meta_path = self.model_dir / "lstm_meta.json"
        
        self.load()
    
    def prepare_sequences(self, data: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ì¤€ë¹„ (GPU ê°€ì†)"""
        if len(data) < self.sequence_length + self.prediction_horizon:
            return None, None
        
        # ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ GPUì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ
        sequences_x = []
        sequences_y = []
        
        for i in range(len(data) - self.sequence_length - self.prediction_horizon + 1):
            # ì…ë ¥: ê³¼ê±° 30ê°œ ì‹œì ì˜ NB Wave + ê°€ê²©
            seq_x = []
            for j in range(i, i + self.sequence_length):
                card = data[j].get('card', {})
                nb = card.get('nb', {})
                
                # Feature ì¶”ì¶œ (ë¹ ë¥¸ ê³„ì‚°)
                p_max = float(nb.get('price', {}).get('max', 0))
                p_min = float(nb.get('price', {}).get('min', 0))
                v_max = float(nb.get('volume', {}).get('max', 0))
                v_min = float(nb.get('volume', {}).get('min', 0))
                t_max = float(nb.get('turnover', {}).get('max', 0))
                t_min = float(nb.get('turnover', {}).get('min', 0))
                
                current_price = float(card.get('current_price', 0))
                zone_flag = float(card.get('insight', {}).get('zone_flag', 0))
                
                seq_x.append([p_max, p_min, v_max, v_min, t_max, t_min, current_price, zone_flag])
            
            sequences_x.append(seq_x)
            
            # ì¶œë ¥: ë¯¸ë˜ 10ê°œ ì‹œì ì˜ zone + ê°€ê²©
            seq_y = []
            for j in range(i + self.sequence_length, i + self.sequence_length + self.prediction_horizon):
                if j < len(data):
                    card = data[j].get('card', {})
                    future_price = float(card.get('current_price', 0))
                    future_zone = float(card.get('insight', {}).get('zone_flag', 0))
                    seq_y.append([future_zone, future_price])
                else:
                    break
            
            if len(seq_y) == self.prediction_horizon:
                sequences_y.append(seq_y)
            else:
                sequences_x.pop()  # ë¶ˆì™„ì „í•œ ì‹œí€€ìŠ¤ ì œê±°
        
        if not sequences_x or not sequences_y:
            return None, None
        
        return np.array(sequences_x), np.array(sequences_y)
    
    def build_model(self, input_shape):
        """LSTM ëª¨ë¸ êµ¬ì¶• (GPU ìµœì í™”)"""
        # GPU ìˆì„ ê²½ìš° ë” í° ëª¨ë¸ ì‚¬ìš©
        if USE_GPU:
            model = Sequential([
                Input(shape=input_shape),
                LSTM(256, return_sequences=True, activation='relu'),
                Dropout(0.3),
                LSTM(128, return_sequences=True, activation='relu'),
                Dropout(0.3),
                LSTM(64, return_sequences=False, activation='relu'),
                Dropout(0.2),
                Dense(128, activation='relu'),
                Dropout(0.2),
                Dense(64, activation='relu'),
                Dense(self.prediction_horizon * 2)  # 10ê°œ ì‹œì  * (zone + price)
            ])
            logger.info("[LSTM] ğŸš€ GPU ìµœì í™” ëª¨ë¸ ìƒì„± (í° ëª¨ë¸)")
        else:
            # CPU ë²„ì „: ì‘ì€ ëª¨ë¸
            model = Sequential([
                Input(shape=input_shape),
                LSTM(128, return_sequences=True),
                Dropout(0.2),
                LSTM(64, return_sequences=False),
                Dropout(0.2),
                Dense(64, activation='relu'),
                Dropout(0.2),
                Dense(self.prediction_horizon * 2)
            ])
            logger.info("[LSTM] CPU ëª¨ë¸ ìƒì„± (ì†Œí˜• ëª¨ë¸)")
        
        # GPU ê²½ìš° ë” ë†’ì€ í•™ìŠµë¥  ì‚¬ìš© ê°€ëŠ¥
        learning_rate = 0.001 if USE_GPU else 0.0005
        
        model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='mse',
            metrics=['mae'],
            run_eagerly=False  # GPU ìµœì í™”
        )
        
        return model
    
    def train(self, training_data: List[Dict]) -> Dict:
        """LSTM ëª¨ë¸ í›ˆë ¨"""
        logger.info("[LSTM] í›ˆë ¨ ì‹œì‘")
        
        if not TF_AVAILABLE:
            logger.error("[LSTM] TensorFlow ì—†ìŒ")
            return {"ok": False, "error": "TensorFlow not available"}
        
        # ì‹œí€€ìŠ¤ ì¤€ë¹„
        X, y = self.prepare_sequences(training_data)
        
        if X is None or y is None:
            logger.warning(f"[LSTM] ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨: ë°ì´í„° {len(training_data)}ê°œ")
            return {"ok": False, "error": "insufficient data for sequences"}
        
        if len(X) < 10:
            logger.warning(f"[LSTM] ì‹œí€€ìŠ¤ ë¶€ì¡±: {len(X)}ê°œ")
            return {"ok": False, "error": f"not enough sequences (need >=10, got {len(X)})"}
        
        logger.info(f"[LSTM] ì‹œí€€ìŠ¤: {len(X)}ê°œ (ì…ë ¥ shape: {X.shape}, ì¶œë ¥ shape: {y.shape})")
        
        # GPU ê°€ì† ìŠ¤ì¼€ì¼ë§ (TensorFlow ops ì‚¬ìš©)
        n_samples, n_timesteps, n_features = X.shape
        
        if USE_GPU and TF_AVAILABLE:
            logger.info("[LSTM] ğŸš€ GPU ê°€ì† ìŠ¤ì¼€ì¼ë§ ì‹œì‘")
            
            # TensorFlow Tensorë¡œ ë³€í™˜ (ìë™ìœ¼ë¡œ GPU ì‚¬ìš©)
            X_tf = tf.constant(X, dtype=tf.float32)
            y_tf = tf.constant(y, dtype=tf.float32)
            
            # GPUì—ì„œ ì •ê·œí™” (ìˆ˜ë™ MinMax)
            X_reshaped = tf.reshape(X_tf, [-1, n_features])
            X_min = tf.reduce_min(X_reshaped, axis=0)
            X_max = tf.reduce_max(X_reshaped, axis=0)
            X_range = X_max - X_min + 1e-8
            
            X_scaled = (X_reshaped - X_min) / X_range
            X_scaled = tf.reshape(X_scaled, [n_samples, n_timesteps, n_features])
            
            y_reshaped = tf.reshape(y_tf, [-1, 2])
            y_min = tf.reduce_min(y_reshaped, axis=0)
            y_max = tf.reduce_max(y_reshaped, axis=0)
            y_range = y_max - y_min + 1e-8
            
            y_scaled = (y_reshaped - y_min) / y_range
            y_scaled = tf.reshape(y_scaled, [n_samples, -1])
            
            # NumPyë¡œ ë³€í™˜ (í•™ìŠµìš©)
            X_scaled = X_scaled.numpy()
            y_scaled = y_scaled.numpy()
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì—­ë³€í™˜ìš©)
            self.scaler_x_min = X_min.numpy()
            self.scaler_x_range = X_range.numpy()
            self.scaler_y_min = y_min.numpy()
            self.scaler_y_range = y_range.numpy()
            
            logger.info("[LSTM] âœ“ GPU ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
        else:
            logger.info("[LSTM] CPU ìŠ¤ì¼€ì¼ë§")
            
            # CPU: sklearn ì‚¬ìš©
            X_reshaped = X.reshape(-1, n_features)
            self.scaler_x = MinMaxScaler()
            X_scaled = self.scaler_x.fit_transform(X_reshaped)
            X_scaled = X_scaled.reshape(n_samples, n_timesteps, n_features)
            
            y_reshaped = y.reshape(-1, 2)
            self.scaler_y = MinMaxScaler()
            y_scaled = self.scaler_y.fit_transform(y_reshaped)
            y_scaled = y_scaled.reshape(n_samples, -1)
        
        # Train/Test split
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_scaled, test_size=0.2, random_state=42
        )
        
        # ëª¨ë¸ êµ¬ì¶•
        self.model = self.build_model((n_timesteps, n_features))
        logger.info(f"[LSTM] ëª¨ë¸ êµ¬ì¡°: {X_train.shape} â†’ {y_train.shape}")
        
        # GPU ìµœì í™” í›ˆë ¨ íŒŒë¼ë¯¸í„°
        if USE_GPU:
            epochs = 100
            batch_size = 64  # GPUëŠ” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆ íš¨ìœ¨ì 
            logger.info("[LSTM] ğŸš€ GPU ëª¨ë“œ: epochs=100, batch_size=64")
        else:
            epochs = 50
            batch_size = 16
            logger.info("[LSTM] CPU ëª¨ë“œ: epochs=50, batch_size=16")
        
        # í›ˆë ¨
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=batch_size,
            verbose=0
        )
        
        # í‰ê°€
        train_loss = history.history['loss'][-1]
        test_loss = history.history['val_loss'][-1]
        train_mae = history.history['mae'][-1]
        test_mae = history.history['val_mae'][-1]
        
        self.meta = {
            "trained_at": datetime.now().isoformat(),
            "train_count": len(X),
            "train_loss": float(train_loss),
            "test_loss": float(test_loss),
            "train_mae": float(train_mae),
            "test_mae": float(test_mae),
            "sequence_length": self.sequence_length,
            "prediction_horizon": self.prediction_horizon,
            "gpu_enabled": USE_GPU,
            "epochs": epochs,
            "batch_size": batch_size
        }
        
        self.save()
        
        logger.info(f"[LSTM] âœ“ í›ˆë ¨ ì™„ë£Œ")
        logger.info(f"[LSTM]   Train Loss: {train_loss:.4f}, MAE: {train_mae:.4f}")
        logger.info(f"[LSTM]   Test Loss: {test_loss:.4f}, MAE: {test_mae:.4f}")
        
        return {
            "ok": True,
            "train_loss": float(train_loss),
            "test_loss": float(test_loss),
            "train_mae": float(train_mae),
            "test_mae": float(test_mae),
            "train_count": len(X)
        }
    
    def predict(self, sequence_data: List[Dict]) -> Dict:
        """ë¯¸ë˜ Zone + ê°€ê²© ì˜ˆì¸¡"""
        if not TF_AVAILABLE or self.model is None:
            return {"ok": False, "error": "model not available"}
        
        if len(sequence_data) < self.sequence_length:
            return {"ok": False, "error": f"need {self.sequence_length} sequence points"}
        
        # ìµœê·¼ 30ê°œ ì‹œí€€ìŠ¤ ì¤€ë¹„
        seq_x = []
        recent = sequence_data[-self.sequence_length:]
        
        for data in recent:
            card = data.get('card', {})
            nb = card.get('nb', {})
            
            p_max = float(nb.get('price', {}).get('max', 0))
            p_min = float(nb.get('price', {}).get('min', 0))
            v_max = float(nb.get('volume', {}).get('max', 0))
            v_min = float(nb.get('volume', {}).get('min', 0))
            t_max = float(nb.get('turnover', {}).get('max', 0))
            t_min = float(nb.get('turnover', {}).get('min', 0))
            
            current_price = float(card.get('current_price', 0))
            zone_flag = float(card.get('insight', {}).get('zone_flag', 0))
            
            seq_x.append([p_max, p_min, v_max, v_min, t_max, t_min, current_price, zone_flag])
        
        X = np.array([seq_x])
        
        # GPU ê°€ì† ìŠ¤ì¼€ì¼ë§ ë° ì˜ˆì¸¡
        if USE_GPU and TF_AVAILABLE and hasattr(self, 'scaler_x_min'):
            logger.debug("[LSTM] ğŸš€ GPU ê°€ì† ì˜ˆì¸¡")
            
            # TensorFlowì—ì„œ ì •ê·œí™”
            X_tf = tf.constant(X, dtype=tf.float32)
            X_reshaped = tf.reshape(X_tf, [-1, X.shape[-1]])
            X_scaled = (X_reshaped - self.scaler_x_min) / (self.scaler_x_range + 1e-8)
            X_scaled = tf.reshape(X_scaled, [1, self.sequence_length, -1])
            
            # ì˜ˆì¸¡ (GPUì—ì„œ ìˆ˜í–‰)
            y_pred = self.model.predict(X_scaled, verbose=0)
            
            # GPUì—ì„œ ì—­ì •ê·œí™”
            y_pred_tf = tf.constant(y_pred, dtype=tf.float32)
            y_pred_reshaped = tf.reshape(y_pred_tf, [-1, 2])
            y_inversed = (y_pred_reshaped * (self.scaler_y_range + 1e-8)) + self.scaler_y_min
            y_inversed = y_inversed.numpy()
            
        else:
            # CPU ì˜ˆì¸¡
            X_reshaped = X.reshape(-1, X.shape[-1])
            if hasattr(self, 'scaler_x'):
                X_scaled = self.scaler_x.transform(X_reshaped)
            else:
                X_scaled = X_reshaped  # ìŠ¤ì¼€ì¼ëŸ¬ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            X_scaled = X_scaled.reshape(1, self.sequence_length, -1)
            
            # ì˜ˆì¸¡
            y_pred = self.model.predict(X_scaled, verbose=0)
            
            # ì—­ìŠ¤ì¼€ì¼ë§
            y_pred_reshaped = y_pred.reshape(-1, 2)
            if hasattr(self, 'scaler_y'):
                y_inversed = self.scaler_y.inverse_transform(y_pred_reshaped)
            else:
                y_inversed = y_pred_reshaped
        
        # ê²°ê³¼ íŒŒì‹±
        predictions = []
        for i in range(self.prediction_horizon):
            zone_flag = int(np.clip(y_inversed[i][0], -1, 1))
            price = float(y_inversed[i][1])
            
            # NB value ê³„ì‚° (zoneì— ë”°ë¼)
            if zone_flag > 0:
                nb_value = 0.6  # BLUE
            elif zone_flag < 0:
                nb_value = 0.4  # ORANGE
            else:
                nb_value = 0.5  # NEUTRAL
            
            predictions.append({
                "index": i,
                "zone_flag": zone_flag,
                "zone": "BLUE" if zone_flag > 0 else "ORANGE" if zone_flag < 0 else "NEUTRAL",
                "predicted_price": price,
                "nb_value": nb_value,
                "confidence": 0.7  # LSTM ê¸°ë³¸ ì‹ ë¢°ë„
            })
        
        return {
            "ok": True,
            "predictions": predictions,
            "count": len(predictions)
        }
    
    def save(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            saved_files = []
            
            if self.model and TF_AVAILABLE:
                self.model.save(str(self.model_path))
                if self.model_path.exists():
                    size = self.model_path.stat().st_size
                    saved_files.append(f"{self.model_path.name} ({size} bytes)")
            
            if self.scaler_x:
                with open(self.scaler_x_path, 'wb') as f:
                    pickle.dump(self.scaler_x, f)
                if self.scaler_x_path.exists():
                    size = self.scaler_x_path.stat().st_size
                    saved_files.append(f"{self.scaler_x_path.name} ({size} bytes)")
            
            if self.scaler_y:
                with open(self.scaler_y_path, 'wb') as f:
                    pickle.dump(self.scaler_y, f)
                if self.scaler_y_path.exists():
                    size = self.scaler_y_path.stat().st_size
                    saved_files.append(f"{self.scaler_y_path.name} ({size} bytes)")
            
            if self.meta:
                with open(self.meta_path, 'w', encoding='utf-8') as f:
                    json.dump(self.meta, f, indent=2, ensure_ascii=False)
                if self.meta_path.exists():
                    size = self.meta_path.stat().st_size
                    saved_files.append(f"{self.meta_path.name} ({size} bytes)")
            
            logger.info(f"[LSTM] ì €ì¥ ì™„ë£Œ: {', '.join(saved_files)}")
        except Exception as e:
            logger.error(f"[LSTM] ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_path.exists() and TF_AVAILABLE:
                self.model = keras.models.load_model(str(self.model_path))
            
            if self.scaler_x_path.exists():
                with open(self.scaler_x_path, 'rb') as f:
                    self.scaler_x = pickle.load(f)
            
            if self.scaler_y_path.exists():
                with open(self.scaler_y_path, 'rb') as f:
                    self.scaler_y = pickle.load(f)
            
            if self.meta_path.exists():
                with open(self.meta_path, 'r', encoding='utf-8') as f:
                    self.meta = json.load(f)
            
            if self.model:
                logger.info(f"[LSTM] ë¡œë“œ ì™„ë£Œ: {self.meta.get('trained_at', 'unknown')}")
        except Exception as e:
            logger.error(f"[LSTM] ë¡œë“œ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_lstm_model = None


def get_lstm_model() -> LSTMPredictionModel:
    """ì „ì—­ LSTM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _lstm_model
    if _lstm_model is None:
        _lstm_model = LSTMPredictionModel()
    return _lstm_model


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    model = LSTMPredictionModel()
    print(f"\nëª¨ë¸ ìƒíƒœ:")
    print(f"  Loaded: {model.model is not None}")
    print(f"  Meta: {json.dumps(model.meta, indent=2, ensure_ascii=False)}")
